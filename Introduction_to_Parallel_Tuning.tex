\documentclass[a4paper, 12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{array}
\usepackage{fixltx2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{a4wide}
\usepackage{multicol}
\usepackage[table]{xcolor}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage[nottoc]{tocbibind}
\usepackage{listings}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[a4paper,top=1.60cm,bottom=1.60cm,left=1.37cm,right=1.45cm]{geometry}
\usepackage[square, sort, comma, numbers]{natbib}

\onehalfspacing
\graphicspath{{./img/}}
\makeindex

\usepackage{tikz}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}

\tikzset{
>=stealth',
  punktchain/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=8.3em,
    minimum height=1em,
    text centered,
    on chain},
  fillchain/.style={
    rectangle,
    rounded corners,
    fill=blue!30,
    draw=black, very thick,
    text width=8.3em,
    minimum height=1em,
    text centered,
    on chain},
  line/.style={draw, thick, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=4em,
    draw=blue!40!black!90, very thick,
    text width=8.3em,
    minimum height=1.5em,
    text centered,
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

\lstset{
    basicstyle=\scriptsize,
    morekeywords={either,or,transform,rule,to,from,through,function},
    frame=L,
}

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

\begin{document}

\hypersetup{backref,pdfpagemode=FullScreen,colorlinks=true}

\thispagestyle{empty}
\begin{center}
    \vspace*{4cm}
    \textbf{\Large{An Introduction to \\Parallel Tuning}}\\

    \vskip 1cm

    Pedro \textsc{Bruel}

    \emph{phrb@ime.usp.br}

    \vfill
    \normalsize{\emph{DCC - IME\\
    Universidade de São Paulo}\\}
    \normalsize{São Paulo, \today}
\end{center}

\newpage

\tableofcontents

\newpage

\section{Introduction} \label{sec:intro}

A number of factors make programming \textbf{H}igh-\textbf{P}erformance
\textbf{C}omputing applications a very complex task. To achieve high
performance it is necessary to write programs that take into consideration
the specifics of a parallel architecture, as well as the basic concepts
of parallel programming. Parallel computing platforms are increasingly
available and heterogeneous. Processors are now expected to be multi-core,
and even personal computers have coprocessors and accelerators such as
GPUs. Consequently, it is easy to over-optimize a parallel program to
a specific parallel computing platform.

This leads to highly efficient hand-optimized programs, capable of leveraging
the potential of a specific parallel computing platform. Although, despite the
efforts made during implementation, this highly-optimized program will not able
to achieve the same performance in a different parallel architecture. In other
words, the performance achieved by extensive hand-optimization is
not \emph{portable}. The lack of performance portability of HPC applications,
as well as the immense efforts needed to optimize them, justify the research
for automated methods of optimization for parallel programs.

Extensive research in the last decade produced powerful tools for
automatically tuning programs, or \emph{autotuners}, in a variety of problem
domains. Despite being able to achieve good results on porting the performance
of parallel programs,these tools are not widely utilized by the scientific 
and programming communities, perhaps because of their novelty.

Autotuners can be model-based or empirical. Model-based autotuners predict 
program performance following a model of the target architecture. The model
is built previous to the tuning and execution, during an \emph{installation}
phase. Empirical autotuners discover the best optimization of a program by
executing different optimized versions and measuring their performance. No
model is built or used, instead empirical autotuner commonly use search
techniques to explore the space defined by the possible optimizations 
of a program, in a given architecture. Despite being arguably slower than
model-base autotuners, given a model is already built, empirical autotuners
can produce optimized versions of a program during tuning time. The empirical
tuning strategy allows programmers to focus their efforts in designing
programs that explicitly expose their implementation and optimization choices,
leaving the task of discovering the best choice to the autotuner.

The search techniques used by an autotuner can target the optimization of 
various program metrics. Simple metrics such as runtime can be obtained by 
simply running the candidate optimization, but more complex metrics, such 
as I/O patterns or memory accesses are not so straight-forward to measure.
\emph{Profilers} are tools that offer such specific measurements and analysis 
of programs during runtime. For instance, if a programmer was interested in 
optimizing the CPU load, or the duration and depth of a function call stack 
during the execution of her program, she could use a profiler to produce such 
measurements to an empirical autotuner.

This Introduction to Parallel Tuning aims to familiarize the reader with the
state-of-the-art applications of autotuning techniques to the optimization
of high-performance parallel programs. The following section discuss the
autotuning technique under the Algorithm Selection framework. Section 
\ref{sec:insieme} discusses the INSIEME compiler project for parallel
applications. Section \ref{sec:opentuner} describes the OpenTuner framework,
an autotuning framework that can be used to implement autotuners for parallel
applications. Section \ref{sec:profilers} list some profiling tools that can
be used to measure metrics of parallel programs. Section \ref{sec:related}
discusses some of the recent work on parallel automatic optimization.
Finally, section \ref{sec:conclusion} summarizes the discussion.

\section{Autotuning} \label{sec:autotuning}

The idea behind autotuning is to use the performance-impacting features 
of architectures, problems and algorithms in a domain to describe
a \emph{search space}. Searching, optimization and Machine Learning techniques
tailored for an application domain are then used to empirically
search this space for optimal algorithm-to-problem mappings.

The autotuning technique has been used since as early as 1997, when the PHiPAC
system~\cite{bilmes1997phipac} used code generators and search scripts to
automatically generate high performance code for matrix multiplication. Since
then the autotuning problem has been tackled in multiple domains, with a great
variety of strategies. Whaley \emph{et al.}~\cite{whaley1998atlas} introduce the
ATLAS project, that produced optimized dense matrix multiply routines. The
OSKI~\cite{vuduc2005oski} library provides automatically tuned kernels for
sparse matrices. The FFTW~\cite{frigo1998fftw} library provides tuned C
subroutines for computing the Discrete Fourier Transform.

More recently, there have been efforts in the implementation of generic tools
for autotuning. PetaBricks~\cite{ansel2009petabricks} is a language, compiler
and autotuner for generic programs, that introduce new abstractions such as the
\emph{either...or} keywords, which let programmers define multiple algorithms
for a same problem. The OpenTuner framework~\cite{ansel2014opentuner} implements
ensembles of search techniques that are used to search a user-defined space of
program configurations. The OpenTuner framework has already been used to
implement a domain specific language for data-flow programming
\cite{bosboom2014streamjit} and a framework for the optimization of recursive
parallel algorithms~\cite{eliahu2015frpa}. The ParamILS
framework~\cite{hutter2009paramils} implements state-of-the-art search methods
for algorithm configuration and parameter tuning.

Some of the work on GPU autotuning~\cite{guo2010autotuningCUDA,
dongarra2009note,grauer2012autotuningHLLGPU} aim to tune parameters such as block
sizes, tiling techniques, transfers between single and double precision, loop
permutations and unrolling. They worked under the assumption that these
parameters were enough to capture the algorithmic design space and attain a
large fraction of peak performance.

In an effort to provide a common representation of multiple parallel
programming models, the INSIEME compiler project~\cite{jordan2012multi}
implements abstractions for OpenMP, MPI and OpenCL. The INSIEME compiler is
able to generate optimized parallel code for heterogeneous multi-core
architectures.

\subsection{The Algorithm Selection Problem}

The description of the Algorithm Selection Problem was first published by Rice
in 1976~\cite{rice1976algorithm}. The problem consists in, given a set of
\emph{algorithms} and a set of \emph{problems}, finding a \emph{mapping} of
algorithms to problems that minimizes the time to solve all problems in the
set. The \emph{algorithms} that compose a set can represent different
abstractions, such as programs, heuristics, or configurations. The set of
\emph{problems} usually contains instances of a problem.

The Algorithm Selection Problem is hard. Its NP-completeness has been proved
when calculating static distributions of algorithms in parallel machines
\cite{bougeret2009combining}. Its Undecidability in the general case was also
shown~\cite{guo2003algorithm}.

\section{INSIEME} \label{sec:insieme}

\section{OpenTuner} \label{sec:opentuner}

\section{Profiling Tools} \label{sec:profilers}

- Add table of parallel and GPU profilers.


\section{Related Work} \label{sec:related}

\subsection{FRPA}

\subsection{GPU tunning*}

\subsection{Compiler Parameters*}

\section{Conclusion} \label{sec:conclusion}

\bibliographystyle{plainnat} 
\bibliography{Introduction_to_Parallel_Tuning}

\end{document}
